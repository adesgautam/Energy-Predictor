# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iHIDLXGDekQrSY1YHtMhBuRbApFJf9NQ
"""

from math import sqrt
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error

import keras
from keras.layers import Dense
from keras.models import Sequential, load_model
from keras.utils import to_categorical
from keras.optimizers import SGD 
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
import itertools
from keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
from keras.callbacks import ModelCheckpoint

import requests 
import json
from datetime import datetime
import pytz

url = "http://api.eia.gov/series/?api_key=31c285edf7b11b33e6703e95c142d66c&series_id=EBA.FLA-ALL.D.H"
r = requests.get(url).text

data = json.loads(r)
data = data["series"][0]["data"]
dataset = pd.DataFrame(data, columns=['DateTime', 'Energy (mWh)'])
dataset['DateTime'] = pd.to_datetime(dataset['DateTime'], infer_datetime_format=True)
last_time = dataset.iloc[0,:]
dataset = dataset[::-1]
dataset.set_index('DateTime', inplace=True)
dataset = dataset.astype('float64')
# dataset.dtypes
print("No. of samples",len(dataset))
print(last_time)


## finding all columns that have nan:
dropping_list_all=[]
for j in range(0,1):
	if not dataset.iloc[:, j].notnull().all():
		dropping_list_all.append(j)        
		#print(df.iloc[:,j].unique())
print(dropping_list_all)

# filling nan with mean in any columns
for j in range(0,1):        
	dataset.iloc[:,j]=dataset.iloc[:,j].fillna(dataset.iloc[:,j].mean())
        
# another sanity check to make sure that there are not more any nan
print(dataset.isnull().sum())

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	dff = pd.DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(dff.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(dff.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = pd.concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

values = dataset.values
timesteps = 1

# normalize features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)

# frame as supervised learning
reframed = series_to_supervised(scaled, timesteps, 1)
n_train_time = int(len(reframed) *(0.8))
# reframed

# split into train and test sets
values = reframed.values

train = values[:n_train_time, :]
test = values[n_train_time:, :]

# split into input and outputs
train_X, train_y = train[:, :-1], train[:, -1]
test_X, test_y = test[:, :-1], test[:, -1]
print(train_X.shape, test_X.shape)

features = 1
train_X = train_X.reshape((train_X.shape[0], timesteps, features))
test_X = test_X.reshape((test_X.shape[0], timesteps, features))
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) 

# shape [samples, timesteps, features].

# Stateless Model
batch_size=1
# stateless model
model = Sequential()
model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(25))
model.add(Dense(1))

model.compile(loss='mean_squared_error', optimizer='adam')
model.summary()

filepath="ergpred-{epoch:02d}-{loss:.4f}.h5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]

# fit network
history = model.fit(train_X, train_y, epochs=5, batch_size=batch_size, validation_data=(test_X, test_y), verbose=2, shuffle=False)

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

# make a prediction
yhat = model.predict(test_X, batch_size=batch_size)
print(yhat.shape, test_X.shape)
test_X = test_X.reshape((test_X.shape[0], features*timesteps))

# invert scaling for forecast
inv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat = inv_yhat[:,0]

# invert scaling for actual
test_y = test_y.reshape((len(test_y), 1))
inv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y = inv_y[:,0]
# print(inv_yhat)

# calculate RMSE
rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))
print('Test RMSE: %.3f' % rmse)

days = 5
hours = 10

aa=[x for x in range(hours)]
plt.plot(aa, inv_y[:hours], marker='.', label="actual")
plt.plot(aa, inv_yhat[:hours], 'r', label="prediction")
plt.ylabel('Energy', size=15)
plt.xlabel('Time step', size=15)
plt.legend(fontsize=15)
plt.show()
print(inv_yhat[:hours])









